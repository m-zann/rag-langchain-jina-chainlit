# Multimodal PDF AssistantÂ ğŸ“„ğŸ¤–

A **Retrievalâ€‘Augmented Generation (RAG)** pipeline that turns any folder of PDFsâ€”manuals, reports, invoices, even scanned paperworkâ€”into a searchable knowledge base.  
Ask questions in a **Chainlit** chat and get answers with cited snippets *and* thumbnails of relevant diagrams.  
It has been created to handle home appliances manuals without having to constantly search for info in paper ones or in pdf files.  
Originally the choice of Jina has been made to leverage its multicontent embeddings, unfortunately local machine couldn't run it and API has some limitations, so we had to rely on OCR for non textual content.

---

## âœ¨  What You Get

| Stage | Tech | Purpose |
|-------|------|---------|
| **Ingest** | `pdf2image`, `pytesseract` | Extract native text, OCR scanned pages (300Â DPI), save hiâ€‘res JPEGs, embed tiny thumbnails |
| **Embeddings** | **JinaÂ Embeddings v4** | Lightweight 8â€¯kâ€‘token context window, great for multimodal payloads |
| **Vector DB** | `Chroma` | Local onâ€‘disk store (`./db`)â€”fast & private |
| **LLM** | `mistral` via **Ollama** | Runs fully offline, GPUâ€‘accelerated |
| **UI** | **Chainlit** | Modern chat with expandable sources & inline images |

---

## ğŸ›   Prerequisites

| Component | Why | Install |
|-----------|-----|---------|
| **PythonÂ â‰¥â€¯3.9** | Runtime | <https://www.python.org/downloads/> |
| **Ollama** | Local LLM backend | <https://ollama.com/download>
| **`mistral` model** | Default 7â€¯B model | `ollama pull mistral` *(after installing Ollama)* |
| **Tesseractâ€‘OCR** | Accurate OCR for scanned PDFs | â€¢ Windows: "[Tesseract at UB Mannheim](https://github.com/UB-Mannheim/tesseract/wiki)" â†’ install, then add the *installation folder* to `%PATH%` or set `TESSERACT_EXE=C:\Program Files\Tesseract-OCR\tesseract.exe` in `.env`.  
â€¢ macOS: `brew install tesseract`  
â€¢ Linux (Debian/Ubuntu): `sudo apt install tesseract-ocr` |
| **Jina Embeddings** | Text & image embeddings | *Two* options â†’ see below |

### ğŸ”‘  OptionÂ A â€” Jina Cloud API  *(simplest)*
1. Sign up at <https://jina.ai> â†’ *DashboardÂ â€º API Keys*.
2. Copy the key into your local `.env`:
   ```env
   JINA_API_KEY=sk-â€¦
   ```
3. **Limitation:** Cloud API enforces an *~8â€¯kâ€‘token* total payload. To stay under the cap we create aggressivelyâ€‘compressed thumbnails instead of fullâ€‘size images.

### ğŸ–¥ï¸  OptionÂ B â€” Run Jina Locally *(no token limits)*
1. Follow the [official guide](https://github.com/jina-ai/embeddings) to spin up a local server (Dockerâ€‘compose or binary).
2. Remove/comment `JINA_API_KEY` from `.env`.
3. Because *local* doesnâ€™t impose the 8â€¯k cap, you can **set `THUMB_QUALITY=95`** in `.env` (or even disable thumbnails) and keep fullâ€‘resolution images; OCR still runs for scanned pages.

---

## ğŸš€  Installation & First Run

> The following assumes **Unixâ€‘like shell**. For WindowsÂ CMD/Powershell just translate `source` â†’ `venv\Scripts\activate` and use backslashes.

```bash
# 1ï¸âƒ£  Clone & enter
$ git clone https://github.com/m-zann/rag-langchain-jina-chainlit.git
$ cd pdfâ€‘assistant

# 2ï¸âƒ£  Create & activate a virtualenv (recommended)
$ python -m venv .venv
$ source .venv/bin/activate

# 3ï¸âƒ£  Install project in editable mode
$ pip install -e .

# 4ï¸âƒ£  Copy env template and fill in the blanks
$ cp .env.example .env
#    -> add JINA_API_KEY (or not)
#    -> set TESSERACT_EXE on Windows if needed

# 5ï¸âƒ£  Drop your PDFs
$ mkdir -p data/pdfs
$ mv ~/Downloads/*.pdf data/pdfs/

# 6ï¸âƒ£  Build / update the vector DB (can be rerun any time)
$ python -m assistant.ingest  # takes a few minutes depending on pages

# 7ï¸âƒ£  Chat!
$ chainlit run chainlit_app/main.py
```

**ProÂ tip:** `assistant.ingest` is idempotent; it only reâ€‘processes new or modified PDFs.

---

## ğŸ’¬  Usage Patterns

### Chainlit UI
* Type naturalâ€‘language questions like **â€œWhat does the error light mean on modelÂ XYZ?â€**  
* Click on the *source_0*, *source_1*â€¦ pills to expand the exact PDF snippets, or on *img_0* to display a referenced diagram.

### CLI (headless)
```bash
$ python -m assistant.cli "How to clean the filter on ABC123?"
```
Use the `--ingest` flag to force a fresh ingest right before querying.

---

## ğŸ–¥ï¸  GPU Acceleration (NVIDIA)

Ollama should detect CUDA automatically, but on Windows it might keep running on CPU, making the experience very slow:

1. **Force the GPU** in *NVIDIA Control Panel â†’ Manage 3D Settings â†’ Program Settings* â†’ select `ollama.exe` â†’ set *Highâ€‘performance NVIDIA processor*.
2. Override layers at runtime:
   ```powershell
   ollama run mistral --num-gpu-layers 35  # tweak the number for your VRAM
   ```

---

## ğŸ§ª  Running Tests

Tests are still to be completed, to run the current ones use the following command.

```bash
pytest -q
```

Tests spin up an inâ€‘memory Chroma instance and use small fixture PDFs so they finish in seconds.

---

## ğŸ“œ  License

MITÂ Â©Â 2024Â YourÂ Name or Organisation

